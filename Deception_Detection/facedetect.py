"""facedetect newversion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NG7lLsvBj6nYbq2cY-2NQHrM0yNeEqZN
"""

# 필요한 기능들 import
import cv2
import mediapipe as mp
from datetime import datetime
import numpy as np
from scipy.signal import find_peaks
from scipy.spatial import distance as dist
from fer import FER
import matplotlib.pyplot as plt
import threading
import time
import matplotlib.animation as animation
from collections import deque


# 변수 설정들
MAX_FRAMES = 120
RECENT_FRAMES = int(MAX_FRAMES / 10)
EYE_BLINK_HEIGHT = 0.5
SIGNIFICANT_BPM_CHANGE = 8
LIP_COMPRESSION_RATIO = 0.35
TELL_MAX_TTL = 30
TEXT_HEIGHT = 30
FACEMESH_FACE_OVAL = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109, 10]
EPOCH = datetime.timestamp(datetime.now())
recording = None
tells = dict()
blinks = [False] * MAX_FRAMES
blinks2 = [False] * MAX_FRAMES
hand_on_face = [False] * MAX_FRAMES
hand_on_face2 = [False] * MAX_FRAMES
face_area_size = 0
hr_times = list(range(0, MAX_FRAMES))
hr_values = [400] * MAX_FRAMES
avg_bpms = [0] * MAX_FRAMES
gaze_values = [0] * MAX_FRAMES
emotion_detector = FER(mtcnn=True)
meter = cv2.imread('meter.png')
fig = None
ax = None
line = None
peakpts = None
blink_times = []
blink_counts = []
# MediaPipe Face Mesh 및 Drawing Utils 초기화
mp_face_mesh = mp.solutions.face_mesh
mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands

# 특정 랜드마크 인덱스 (예: 눈, 코, 입)
LANDMARKS_TO_DRAW = [
    33, 133, 362, 263,  # 눈
    1,  # 코 끝
    61, 291  # 입
]

# 얼굴이나 신체의 특정 포인트 강조 하기 위한 것
def draw_selected_landmarks(image, landmarks, indices):
    for index in indices:
        x = int(landmarks.landmark[index].x * image.shape[1])
        y = int(landmarks.landmark[index].y * image.shape[0])
        cv2.circle(image, (x, y), 2, (0, 255, 0), -1)

def draw_face_landmarks(image, landmarks):
    draw_selected_landmarks(image, landmarks, LANDMARKS_TO_DRAW)

def draw_hand_landmarks(image, landmarks):
    for landmark in landmarks.landmark:
        x = int(landmark.x * image.shape[1])
        y = int(landmark.y * image.shape[0])
        cv2.circle(image, (x, y), 2, (255, 0, 0), -1)

# 각각의 점수를 실시간 차트로 그리기 위한 준비
def chart_setup():
    global fig, ax, line, peakpts
    global blink_ax, blink_line
    global ax2  # 진실성 점수 차트 추가

    plt.ion()
    fig, (ax, blink_ax, ax2) = plt.subplots(3, 1, figsize=(10, 12))

    # 심박수 차트 설정
    ax.set(ylim=(0, 150))
    line, = ax.plot(hr_times, hr_values, 'b-')
    peakpts, = ax.plot([], [], 'r+')

    # 블링크 차트 설정
    blink_ax.set_ylim(0, 10)  # 초기 범위 설정
    blink_line, = blink_ax.plot(blink_times, blink_counts, 'g-')

    # 진실성 점수 차트 설정
    ax2.set_title('Truth Score Over Time')
    ax2.set_xlabel('Time (s)')
    ax2.set_ylabel('Truth Score')


# ttl이 0이 되면 제거하는 것
def decrement_tells(tells):
    for key, tell in tells.copy().items():
        if 'ttl' in tell:
            tell['ttl'] -= 1
            if tell['ttl'] <= 0:
                del tells[key]
    return tells
    
# 비디오 파일을 입력 받아서 얼굴 및 손 트래킹을 하고 심박수 및 진실 점수를 실시간으로 그래프로 표시하는 부분
def main():
    global TELL_MAX_TTL
    global recording

    video_path = r'C:\\Users\\User\\Desktop\\01_프로젝트\\음성인식\\기만탐지모델\\downloaded_video.mp4'
    cap = cv2.VideoCapture(video_path)

    global truth_scores, truth_score_times
    truth_scores = []
    truth_score_times = []

    calibrated = False
    calibration_frames = 0

    chart_setup()

    mp_face_mesh = mp.solutions.face_mesh
    mp_hands = mp.solutions.hands

    with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
        with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7) as hands:
            while cap.isOpened():
                success, image = cap.read()
                if not success:
                    break

                calibration_frames += process(image, face_mesh, hands, calibrated, draw=True, bpm_chart=True, fps=cap.get(cv2.CAP_PROP_FPS))
                calibrated = (calibration_frames >= MAX_FRAMES)

                # 'Face' 창은 제거하고 'Face and Hand Tracking'만 표시합니다.
                cv2.imshow('Face and Hand Tracking', image)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

            cap.release()
            cv2.destroyAllWindows()

    plt.show()

# 이벤트 상태 변화를 기록하고 추적하는 시스템
def new_tell(result):
  global TELL_MAX_TTL

  return {
    'text': result,
    'ttl': TELL_MAX_TTL
  }

# 이미지 프레임 위에 얼굴과 손의 랜드마크 그리는 기
def draw_on_frame(image, face_landmarks, hands_landmarks):
     # 얼굴 윤곽선 그리기
    mp.solutions.drawing_utils.draw_landmarks(
          image,
          face_landmarks,
          mp.solutions.face_mesh.FACEMESH_CONTOURS,
          landmark_drawing_spec=None,
          connection_drawing_spec=mp.solutions.drawing_styles
          .get_default_face_mesh_contours_style())
        # 얼굴 홍채 그리기
    mp.solutions.drawing_utils.draw_landmarks(
          image,
          face_landmarks,
          mp.solutions.face_mesh.FACEMESH_IRISES,
          landmark_drawing_spec=None,
          connection_drawing_spec=mp.solutions.drawing_styles
          .get_default_face_mesh_iris_connections_style())
    # 손 랜드마크 그리기
    for hand_landmarks in (hands_landmarks or []):
        mp.solutions.drawing_utils.draw_landmarks(
            image,
            hand_landmarks,
            mp.solutions.hands.HAND_CONNECTIONS,
            mp.solutions.drawing_styles.get_default_hand_landmarks_style(),
            mp.solutions.drawing_styles.get_default_hand_connections_style())



# 이미지 위에 텍스트를 그리기 위한 기능
def write(text, image, x, y):
    cv2.putText(img=image, text=text, org=(x, y),
        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=[0, 0, 0],
        lineType=cv2.LINE_AA, thickness=4)
    cv2.putText(img=image, text=text, org=(x, y),
        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=[255, 255, 255],
        lineType=cv2.LINE_AA, thickness=2)



# 객체의 가로 새로 길이를 파악 하기 위한 기능
def get_aspect_ratio(top, bottom, right, left):
  height = dist.euclidean([top.x, top.y], [bottom.x, bottom.y])
  width = dist.euclidean([right.x, right.y], [left.x, left.y])
  return height / width

# 네개의 점을 이용하여 특정 영역을 추출하여 시각적 표현을 하는 기능
def get_area(image, draw, topL, topR, bottomR, bottomL):
  topY = int((topR.y+topL.y)/2 * image.shape[0])
  botY = int((bottomR.y+bottomL.y)/2 * image.shape[0])
  leftX = int((topL.x+bottomL.x)/2 * image.shape[1])
  rightX = int((topR.x+bottomR.x)/2 * image.shape[1])

  if draw:
    image = cv2.circle(image, (leftX,topY), 2, (255,0,0), 2)
    image = cv2.circle(image, (leftX,botY), 2, (255,0,0), 2)
    image = cv2.circle(image, (rightX,topY), 2, (255,0,0), 2)
    image = cv2.circle(image, (rightX,botY), 2, (255,0,0), 2)

  return image[topY:botY, rightX:leftX]

# 심박수 데이터를 처리하고 업데이트를 하는 기능
def get_bpm_tells(cheekL, cheekR, fps, bpm_chart):
    global hr_times, hr_values, avg_bpms
    global ax, line, peakpts
    global blink_ax, blink_line

    cheekLwithoutBlue = np.average(cheekL[:, :, 1:3])
    cheekRwithoutBlue = np.average(cheekR[:, :, 1:3])
    #심박수 데이터를 업데이트
    hr_values = hr_values[1:] + [cheekLwithoutBlue + cheekRwithoutBlue]

    if not fps:
        hr_times = hr_times[1:] + [time.time() - EPOCH]
    # bpm 차트를 업데이트
    if bpm_chart:
        line.set_data(hr_times, hr_values)
        ax.relim()
        ax.autoscale()

        # 블링크 차트 업데이트
        blink_times.append(time.time() - EPOCH)  # 현재 시간을 블링크 시간으로 추가
        blink_counts.append(sum(blinks))  # 블링크 카운트 추가
        blink_line.set_data(blink_times, blink_counts)
        blink_ax.relim()
        blink_ax.autoscale()
     # 심박수 신호에서 피크를 찾기
    peaks, _ = find_peaks(hr_values, threshold=0.2, distance=10, prominence=1, wlen=20)
    peak_times = [hr_times[i] for i in peaks]

    if bpm_chart:
        peakpts.set_data(peak_times, [hr_values[i] for i in peaks])
     # BPM 계산
    bpms = 60 * np.diff(peak_times) / (fps or 1)
    bpms = bpms[(bpms > 50) & (bpms < 150)]
    recent_bpms = bpms[(-3 * RECENT_FRAMES):]
    # 최근 BPM의 평균 계산
    recent_avg_bpm = 0
    bpm_display = "BPM: ..."
    if recent_bpms.size > 1:
        recent_avg_bpm = int(np.average(recent_bpms))
        bpm_display = "BPM: {} ({})".format(recent_avg_bpm, len(recent_bpms))

    avg_bpms = avg_bpms[1:] + [recent_avg_bpm]
    # BPM 변화 계산
    bpm_delta = 0
    bpm_change = ""

    if len(recent_bpms) > 2:
        all_bpms = list(filter(lambda bpm: bpm != '-', avg_bpms))
        all_avg_bpm = sum(all_bpms) / len(all_bpms)
        avg_recent_bpm = sum(recent_bpms) / len(recent_bpms)
        bpm_delta = avg_recent_bpm - all_avg_bpm

        if bpm_delta > SIGNIFICANT_BPM_CHANGE:
            bpm_change = "Heart rate increasing"
        elif bpm_delta < -SIGNIFICANT_BPM_CHANGE:
            bpm_change = "Heart rate decreasing"

    return bpm_display, bpm_change

blink_count = 0
tempSwitch=1

# 얼굴의 눈 깜박임을 감지하는 기능
def is_blinking(face):
    global blink_count
    global tempSwitch
    eyeR = [face[p] for p in [159, 145, 133, 33]]
    eyeR_ar = get_aspect_ratio(*eyeR)

    eyeL = [face[p] for p in [386, 374, 362, 263]]
    # 눈의 높이와 너비의 비율
    eyeL_ar = get_aspect_ratio(*eyeL)

    eyeA_ar = round((eyeR_ar + eyeL_ar) / 2, 2)  # 소수점 둘째 자리로 반올림
    # 눈 깜박임 감지
    if eyeA_ar < EYE_BLINK_HEIGHT:
        tempSwitch = 0
        return False
    elif tempSwitch == 0:
        blink_count += 1  # Increment blink count when a blink is detected
        tempSwitch = 1
        return True
    else:
        return False

# 눈 깜박임 데이터를 분석하는 기능
def get_blink_tell(blinks):
    if sum(blinks[:RECENT_FRAMES]) < 3:  # not enough blinks for valid comparison
        return None

    # 블링크의 수 확인
    recent_closed = 1.0 * sum(blinks[-RECENT_FRAMES:]) / RECENT_FRAMES
    # 평균 블링 비율 계산
    avg_closed = 1.0 * sum(blinks) / MAX_FRAMES

    if recent_closed > (20 * avg_closed):
        return "Increased blinking"
    elif avg_closed > (20 * recent_closed):
        return "Decreased blinking"
    else:
        return None

# 손에 얼굴이 닿았는지 여부를 확인 하는 기능
def check_hand_on_face(hands_landmarks, face):
  if hands_landmarks:
    face_landmarks = [face[p] for p in FACEMESH_FACE_OVAL]
    face_points = [[[p.x, p.y] for p in face_landmarks]]
    face_contours = np.array(face_points).astype(np.single)

    for hand_landmarks in hands_landmarks:
      hand = []
      for point in hand_landmarks.landmark:
        hand.append( (point.x, point.y) )

      for finger in [4, 8, 20]:
        overlap = cv2.pointPolygonTest(face_contours, hand[finger], False)
        if overlap != -1:
          return True
  return False

# 얼굴의 두 눈에서의 시선 방형을 평균하여 반환
def get_avg_gaze(face):
  gaze_left = get_gaze(face, 476, 474, 263, 362)
  gaze_right = get_gaze(face, 471, 469, 33, 133)
  return round((gaze_left + gaze_right) / 2, 1)

# 주어진 눈과 홍채의 좌표를 바탕으로 시선 방향을 계산
def get_gaze(face, iris_L_side, iris_R_side, eye_L_corner, eye_R_corner):
  iris = (
    face[iris_L_side].x + face[iris_R_side].x,
    face[iris_L_side].y + face[iris_R_side].y,
  )
  eye_center = (
    face[eye_L_corner].x + face[eye_R_corner].x,
    face[eye_L_corner].y + face[eye_R_corner].y,
  )

  gaze_dist = dist.euclidean(iris, eye_center)
  eye_width = abs(face[eye_R_corner].x - face[eye_L_corner].x)
  gaze_relative = gaze_dist / eye_width

  if (eye_center[0] - iris[0]) < 0: # flip along x for looking L vs R
    gaze_relative *= -1

  return gaze_relative

# 시선의 변화가 있는지 감지
def detect_gaze_change(avg_gaze):
  global gaze_values

  gaze_values = gaze_values[1:] + [avg_gaze]
  gaze_relative_matches = 1.0 * gaze_values.count(avg_gaze) / MAX_FRAMES
  if gaze_relative_matches < .01: # looking in a new direction
    return gaze_relative_matches
  return 0

# 얼굴 랜드 마크를 이용 하여 입술의 비율 계산
def get_lip_ratio(face):
  return get_aspect_ratio(face[0], face[17], face[61], face[291])

# 진실성의 미터를 영상에 추가하는 기능
def add_truth_meter(image, tell_count):
  width = image.shape[1]
  sm = int(width / 64)
  bg = int(width / 3.2)

  resized_meter = cv2.resize(meter, (bg,sm), interpolation=cv2.INTER_AREA)
  image[sm:(sm+sm), bg:(bg+bg), 0:3] = resized_meter[:, :, 0:3]

  if tell_count:
    tellX = bg + int(bg/4) * (tell_count - 1) # adjust for always-on BPM
    cv2.rectangle(image, (tellX, int(.9*sm)), (tellX+int(sm/2), int(2.1*sm)), (0,0,0), 2)

# 이미지에서 얼굴을 감지하고 상대적인 면적을 계산하며 반환
def get_face_relative_area(face):
  face_width = abs(max(face[454].x, 0) - max(face[234].x, 0))
  face_height = abs(max(face[152].y, 0) - max(face[10].y, 0))
  return face_width * face_height

# 이미지에서 얼굴과 손을 감지하여 반환
def find_face_and_hands(image_original, face_mesh, hands):
  image = image_original.copy()
  image.flags.writeable = False # pass by reference to improve speed
  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

  faces = face_mesh.process(image)
  hands_landmarks = hands.process(image).multi_hand_landmarks

  face_landmarks = None
  if faces.multi_face_landmarks and len(faces.multi_face_landmarks) > 0:
    face_landmarks = faces.multi_face_landmarks[0] # use first face found

  return face_landmarks, hands_landmarks
  
# 가중치 설정
WEIGHTS = {
    "blinking": 10,
    "bpm_change": 20,
    "hand_on_face": 15,
    "gaze": 15,
    "lips": 10
}

# tell항목에 대한 가중치를 담고 있는 딕셔너리
MAX_SCORE = sum(WEIGHTS.values())

# tells를 이용하여 진실성 점수를 계산하는 기능
def calculate_truth_score(tells):
    score = 0
    for tell in tells:
        if tell in WEIGHTS:
            score += WEIGHTS[tell]

    # 점수 정규화 (0-100)
    truth_score = max(0, 100 - (score / MAX_SCORE) * 100)
    return truth_score
    
# 이미지에서 얼굴과 손의 랜드마크를 분석하고, 생리적 신호 기반으 진실성 점수를 계산 하는 기능
def process(image, face_mesh, hands, calibrated=False, draw=False, bpm_chart=False, flip=False, fps=None):
    global tells
    global blinks, hand_on_face, face_area_size
    global blink_times, blink_counts
    global truth_scores, truth_score_times

    tells = decrement_tells(tells)

    face_landmarks, hands_landmarks = find_face_and_hands(image, face_mesh, hands)
    if face_landmarks:
        face = face_landmarks.landmark
        face_area_size = get_face_relative_area(face)

        # cheekL과 cheekR을 정의하고 초기화
        cheekL = get_area(image, draw, topL=face[449], topR=face[350], bottomR=face[429], bottomL=face[280])
        cheekR = get_area(image, draw, topL=face[121], topR=face[229], bottomR=face[50], bottomL=face[209])

        avg_bpms, bpm_change = get_bpm_tells(cheekL, cheekR, fps, bpm_chart)
        tells['avg_bpms'] = new_tell(avg_bpms)
        if len(bpm_change):
            tells['bpm_change'] = new_tell(bpm_change)

        blinks = blinks[1:] + [is_blinking(face)]
        recent_blink_tell = get_blink_tell(blinks)
        if recent_blink_tell:
            tells['blinking'] = new_tell(recent_blink_tell)

        recent_hand_on_face = check_hand_on_face(hands_landmarks, face)
        hand_on_face = hand_on_face[1:] + [recent_hand_on_face]
        if recent_hand_on_face:
            tells['hand'] = new_tell("Hand covering face")

        avg_gaze = get_avg_gaze(face)
        if detect_gaze_change(avg_gaze):
            tells['gaze'] = new_tell("Change in gaze")

        if get_lip_ratio(face) < LIP_COMPRESSION_RATIO:
            tells['lips'] = new_tell("Lip compression")

        if bpm_chart:
            fig.canvas.draw()
            fig.canvas.flush_events()

        if draw:
            draw_selected_landmarks(image, face_landmarks, LANDMARKS_TO_DRAW)
            if hands_landmarks:
                for hand_landmarks in hands_landmarks:
                    draw_hand_landmarks(image, hand_landmarks)

    if flip:
        image = cv2.flip(image, 1)

    add_text(image, tells, calibrated)

    # 진실성 점수 계산 및 표시
    truth_score = calculate_truth_score(tells)
    truth_scores.append(truth_score)
    truth_score_times.append(time.time() - EPOCH)
    add_truth_meter(image, truth_score)

    # 진실성 점수 그래프 업데이트
    if bpm_chart:
        ax2.clear()  # Clear the previous plot
        ax2.plot(truth_score_times, truth_scores, 'm-')
        ax2.set_title('Truth Score Over Time')
        ax2.set_xlabel('Time (s)')
        ax2.set_ylabel('Truth Score')
        ax2.relim()
        ax2.autoscale_view()

    return 1 if (face_landmarks and not calibrated) else 0


# 진실성의 점수를 영상에 텍스트로 나타냄
def add_truth_meter(image, truth_score):
    cv2.putText(image, f'Truth Score: {truth_score:.2f}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    
# 두 값을 비교하여 특정 비율을 기준으로 반환
def mirror_compare(first, second, rate, less, more):
  if (rate * first) < second:
    return less
  elif first > (rate * second):
    return more
  return None
  
# 두 블링크의 데이터를 집합하여 비교
def get_blink_comparison(blinks1, blinks2):
  return mirror_compare(sum(blinks1), sum(blinks2), 1.8, "Blink less", "Blink more")
  
# 두 손 데이터를 집합하여 비교
def get_hand_face_comparison(hand1, hand2):
  return mirror_compare(sum(hand1), sum(hand2), 2.1, "Stop touching face", "Touch face more")
  
# 두 얼굴의 크기를 비교하여 얼굴이 너무 가까운지 먼지 판단
def get_face_size_comparison(ratio1, ratio2):
  return mirror_compare(ratio1, ratio2, 1.5, "Too close", "Too far")


# process optional second input for mirroring
# 두번째 프레임을 읽고 첫번째 이미지 프레임과 비교하여 분석 하는 기능
def process_second(cap, image, face_mesh, hands):
  global blinks, blinks2
  global hand_on_face, hand_on_face2
  global face_area_size

  success2, image2 = cap.read()
  if success2:
    face_landmarks2, hands_landmarks2 = find_face_and_hands(image2, face_mesh, hands)

    if face_landmarks2:
      face2 = face_landmarks2.landmark

      blinks2 = blinks2[1:] + [is_blinking(face2)]
      blink_mirror = get_blink_comparison(blinks, blinks2)

      hand_on_face2 = hand_on_face2[1:] + [check_hand_on_face(hands_landmarks2, face2)]
      hand_face_mirror = get_hand_face_comparison(hand_on_face, hand_on_face2)

      face_area_size2 = get_face_relative_area(face2)
      face_ratio_mirror = get_face_size_comparison(face_area_size, face_area_size2)

      text_y = 2 * TEXT_HEIGHT # show prompts below 'mood' on right side
      for comparison in [blink_mirror, hand_face_mirror, face_ratio_mirror]:
        if comparison:
          write(comparison, image, int(.75 * image.shape[1]), text_y)
          text_y += TEXT_HEIGHT

def is_blinking(face):
    global blink_count
    global tempSwitch
    eyeR = [face[p] for p in [159, 145, 133, 33]]
    eyeR_ar = get_aspect_ratio(*eyeR)

    eyeL = [face[p] for p in [386, 374, 362, 263]]
    eyeL_ar = get_aspect_ratio(*eyeL)

    eyeA_ar = round((eyeR_ar + eyeL_ar) / 2,0)

    if eyeA_ar < EYE_BLINK_HEIGHT:
        tempSwitch = 0
        return False
    elif  tempSwitch == 0:
        blink_count += 1  # Increment blink count when a blink is detected
        tempSwitch = 1
        return True
    else:
        return False

def add_text(image, tells, calibrated):
    global blink_count

    text_y = TEXT_HEIGHT
    text_x = 10  # 왼쪽 상단에서 시작

    # 블링크 수 표시
    write(f"Blinks: {blink_count}", image, text_x, text_y)
    text_y += TEXT_HEIGHT  # 다음 줄로 이동

    if calibrated:
        for tell in tells.values():
            write(tell['text'], image, text_x, text_y)
            text_y += TEXT_HEIGHT  # 다음 줄로 이동
            
# 데이터 저장을 위한 큐
gaze_data = deque(maxlen=100)  # 최대 100개의 데이터 포인트 저장
blink_data = deque(maxlen=100)
hand_data = deque(maxlen=100)

# 원하는 크기로 조정

if __name__ == '__main__':
    main()

# 기존 데이터 길이 맞추기
import pandas as pd

min_length = min(len(blink_times), len(blink_counts), len(truth_score_times))
blink_times = blink_times[:min_length]
blink_counts = blink_counts[:min_length]
truth_score_times = truth_score_times[:min_length]
truth_scores = truth_scores[:min_length]

# 데이터 프레임 생성
df = pd.DataFrame({
    'Blink Time': blink_times,
    'Blink Count': blink_counts,
    'Truth Score Time': truth_score_times,
    'Truth Score': truth_scores
})

# 데이터 프레임 확인
print(df)

# 데이터 프레임을 CSV 파일로 저장
df.to_csv(r'C:\\Users\\User\\Desktop\\01_프로젝트\\음성인식\\기만탐지모델\\data_with_truth_scores.csv', index=False)
